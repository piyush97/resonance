# ğŸš€ All Services Running - Resonance

## âœ… Services Status

All three services are running successfully:

| Service | Port | Status | URL |
|---------|------|--------|-----|
| **KB Service** | 8000 | âœ… Running | http://localhost:8000 |
| **API Service** | 3001 | âœ… Running | http://localhost:3001 |
| **Web Dashboard** | 3000 | âœ… Running | http://localhost:3000 |
| **Widget Dev** | 5173 | âœ… Running | http://localhost:5173 |

## Health Check

```bash
curl http://localhost:3001/health
```

Response:
```json
{
  "status": "ok",
  "timestamp": "2026-01-20T23:12:43.949Z",
  "services": {
    "api": "ok",
    "kb": "ok",
    "database": "configured"
  }
}
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chat Widget    â”‚  Port 5173 (dev)
â”‚  (React)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ WebSocket
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Service    â”‚  Port 3001
â”‚  (Node.js)      â”‚  - WebSocket server
â”‚                 â”‚  - REST API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Conversation management
         â”‚ HTTP
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KB Service     â”‚  Port 8000
â”‚  (Python)       â”‚  - Document upload
â”‚                 â”‚  - RAG pipeline
â”‚                 â”‚  - Local Ollama LLM
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pinecone (sterling-willow)     â”‚
â”‚  - Integrated embeddings        â”‚
â”‚  - llama-text-embed-v2 (384d)   â”‚
â”‚  - Cosine similarity            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama         â”‚  Port 11434
â”‚  llama3.2       â”‚  - Local LLM
â”‚  (2.0 GB)       â”‚  - GPU accelerated
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## API Endpoints

### KB Service (Port 8000)

```bash
# Health
GET /health

# Upload document
POST /api/knowledge-base/upload
  -F "file=@document.pdf"
  -F "assistant_id=demo"

# Search
POST /api/knowledge-base/search
  {"query": "...", "assistant_id": "demo"}

# Chat
POST /api/knowledge-base/chat
  {"query": "...", "assistant_id": "demo"}
```

### API Service (Port 3001)

```bash
# Health
GET /health

# Create conversation
POST /api/v1/conversations/create
  {"visitor_id": "...", "channel": "web"}

# WebSocket chat
WS /api/v1/conversations/:id/ws

# Assistants
GET    /api/v1/assistants
POST   /api/v1/assistants
GET    /api/v1/assistants/:id
PATCH  /api/v1/assistants/:id
DELETE /api/v1/assistants/:id
```

## Running Services

### Start All Services

```bash
# From project root
npm run dev
```

This starts:
- KB service (Python)
- API service (Node.js)
- Web dashboard (Next.js)
- Widget dev server (Vite)

### Start Individual Services

```bash
# KB Service only
cd services/kb
source venv/bin/activate
python main.py

# API Service only
cd apps/api
npm run dev

# Web Dashboard only
cd apps/web
npm run dev

# Widget only
cd packages/widget
npm run dev
```

## Testing

### Test KB Service

```bash
cd services/kb
source venv/bin/activate
python test_full_pipeline.py
```

### Test API Service

```bash
# Health check
curl http://localhost:3001/health

# Create assistant
curl -X POST http://localhost:3001/api/v1/assistants \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Demo Assistant",
    "description": "Test assistant"
  }'
```

### Test WebSocket

```bash
# Install wscat if needed: npm install -g wscat
wscat -c ws://localhost:3001/api/v1/conversations/test-123/ws

# Send message
{"type": "message", "content": "Hello", "assistant_id": "demo"}
```

## Configuration

### KB Service (`.env`)
```env
EMBEDDING_PROVIDER=pinecone
PINECONE_INDEX_NAME=sterling-willow
LLM_PROVIDER=local
OLLAMA_MODEL=llama3.2:latest
PINECONE_API_KEY=pcsk_...
```

### API Service (`.env`)
```env
PORT=3001
KB_SERVICE_URL=http://localhost:8000
ALLOWED_ORIGINS=*
```

### Web Dashboard (`.env.local`)
```env
NEXT_PUBLIC_API_URL=http://localhost:3001
```

## Cost: Still $0/month!

- Embeddings: FREE (Pinecone integrated, starter plan)
- LLM: FREE (local Ollama)
- Vector DB: FREE (Pinecone free tier)
- Infrastructure: FREE (running locally)

## Next Steps

1. âœ… KB Service working
2. âœ… API Service working
3. â­ï¸ Build web dashboard UI
4. â­ï¸ Integrate widget with API
5. â­ï¸ End-to-end test
6. â­ï¸ Deploy to staging
7. â­ï¸ Create demo materials
8. â­ï¸ Start validation

## Troubleshooting

### Service not starting?
```bash
# Check if ports are in use
lsof -i :3000
lsof -i :3001
lsof -i :8000

# Kill and restart
pkill -f "python main.py"
pkill -f "tsx watch"
npm run dev
```

### WebSocket not connecting?
- Check CORS settings
- Verify API service is running
- Check browser console for errors

---

**Status**: ğŸ‰ All services running!

**Next**: Build dashboard UI and integrate everything
