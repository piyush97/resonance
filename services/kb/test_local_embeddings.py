"""
Test script for local embeddings with sentence-transformers
Tests the full pipeline: model loading, embedding generation, and Pinecone integration
"""
import os
os.environ['EMBEDDING_PROVIDER'] = 'local'
os.environ['LOCAL_EMBEDDING_MODEL'] = 'all-MiniLM-L6-v2'

from sentence_transformers import SentenceTransformer
import torch
import numpy as np

def test_local_embeddings():
    """Test local embedding generation"""
    print("=" * 60)
    print("Testing Local Embeddings (sentence-transformers)")
    print("=" * 60)
    
    # Check GPU
    print("\n1. GPU Check:")
    print(f"   CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # Load model
    print("\n2. Loading Model:")
    print("   Model: all-MiniLM-L6-v2")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print(f"   ‚úì Model loaded")
    print(f"   Dimension: {model.get_sentence_embedding_dimension()}")
    
    # Test embeddings
    print("\n3. Generating Embeddings:")
    texts = [
        "Resonance is an AI customer service chatbot",
        "It helps B2B SaaS companies automate support",
        "The platform uses RAG for accurate responses"
    ]
    
    embeddings = model.encode(texts, show_progress_bar=False)
    print(f"   ‚úì Generated {len(embeddings)} embeddings")
    print(f"   Shape: {embeddings.shape}")
    print(f"   First embedding (5 values): {embeddings[0][:5]}")
    
    # Test similarity
    print("\n4. Testing Similarity:")
    from sentence_transformers.util import cos_sim
    similarity = cos_sim(embeddings[0], embeddings[1])
    print(f"   Similarity between text 1 and 2: {similarity[0][0]:.4f}")
    
    # Performance test
    print("\n5. Performance Test:")
    import time
    test_texts = ["Test sentence"] * 100
    start = time.time()
    _ = model.encode(test_texts, show_progress_bar=False)
    elapsed = time.time() - start
    print(f"   100 embeddings in {elapsed:.2f}s")
    print(f"   Speed: {100/elapsed:.1f} embeddings/sec")
    
    print("\n" + "=" * 60)
    print("‚úÖ All tests passed! Local embeddings working perfectly.")
    print("=" * 60)
    
    return True

def test_ingestion_module():
    """Test the ingestion module with local embeddings"""
    print("\n" + "=" * 60)
    print("Testing Ingestion Module Integration")
    print("=" * 60)
    
    from services.ingestion import get_local_embedding_model, get_embedding_dimension
    
    print("\n1. Getting embedding dimension:")
    dim = get_embedding_dimension()
    print(f"   Dimension: {dim}")
    assert dim == 384, f"Expected 384, got {dim}"
    print("   ‚úì Correct dimension")
    
    print("\n2. Loading model via ingestion module:")
    model = get_local_embedding_model()
    print(f"   ‚úì Model loaded: {type(model).__name__}")
    
    print("\n3. Testing embedding generation:")
    texts = ["Test embedding 1", "Test embedding 2"]
    embeddings = model.encode(texts, show_progress_bar=False)
    print(f"   ‚úì Generated embeddings: {embeddings.shape}")
    
    print("\n" + "=" * 60)
    print("‚úÖ Ingestion module integration working!")
    print("=" * 60)
    
    return True

if __name__ == "__main__":
    try:
        # Test 1: Basic local embeddings
        test_local_embeddings()
        
        # Test 2: Integration with ingestion module
        test_ingestion_module()
        
        print("\n" + "üéâ" * 30)
        print("\nALL TESTS PASSED!")
        print("\nYour local AI stack is ready:")
        print("  ‚úì sentence-transformers 5.2.0")
        print("  ‚úì PyTorch 2.9.1 with GPU support")
        print("  ‚úì Local embeddings (384 dims)")
        print("  ‚úì Integration with ingestion module")
        print("\nCost: $0/month üí∞")
        print("\n" + "üéâ" * 30)
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
